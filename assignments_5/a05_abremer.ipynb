{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aea25c1",
   "metadata": {
    "id": "9aea25c1"
   },
   "source": [
    "# [Assignment 5](https://ovgu-ailab.github.io/idl2023/assignment5.html)\n",
    "\n",
    "Collaborative Work from Adrian Bremer & Philipp Reinig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126b186",
   "metadata": {
    "id": "4126b186"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc6a38e",
   "metadata": {
    "id": "4dc6a38e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada4d4e",
   "metadata": {
    "id": "bada4d4e"
   },
   "source": [
    "## Preparing IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80386305",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80386305",
    "outputId": "0649d1a5-97d3-4fce-8ab0-46bbac3e9b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "num_words = 20000\n",
    "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "j7Ef172nfM80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7Ef172nfM80",
    "outputId": "79bfe888-56a2-4f72-9f90-3388e16e5023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599876ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "599876ed",
    "outputId": "4c143582-bd3c-4571-bb05-051601976b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
    "\n",
    "max_len = max(sequence_lengths)\n",
    "print(max_len)\n",
    "mean_len = int(np.mean(sequence_lengths))\n",
    "print(mean_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bnYsSd3kZ22",
   "metadata": {
    "id": "0bnYsSd3kZ22"
   },
   "source": [
    "**Ideas for using not the full-length padding scheme**\n",
    "- use the mean length and every other word is _UNKNOWN_\n",
    "  - _truncating_ instead of throwing away since the long sequences are important too because when they are longer the way it is written is different & truncating _the back_ (post) because mostly the first few words are like \"Ehh, this is bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ecf2cd",
   "metadata": {
    "id": "01ecf2cd"
   },
   "outputs": [],
   "source": [
    "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_sequences,\n",
    "    maxlen=mean_len, # max_len\n",
    "    padding=\"pre\",\n",
    "    truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Bt0ibtLQDZ7V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bt0ibtLQDZ7V",
    "outputId": "ad0e369a-92d1-40d1-a49d-b471f998f4e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 238)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c359f1ec",
   "metadata": {
    "id": "c359f1ec"
   },
   "outputs": [],
   "source": [
    "# one_hot_sequences_padded = tf.one_hot(indices=train_sequences_padded, depth=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MFdqkgWng7_8",
   "metadata": {
    "id": "MFdqkgWng7_8"
   },
   "source": [
    "**Problem**\n",
    "- the one-hot-vectors are too large so RAM isn't enough\n",
    "\n",
    "**Ideas to fix this**\n",
    "- for storage use integers as vectors because we only need 1 or 0\n",
    "  - here the indices could be used right away\n",
    "- or just use the indices and **construct the one-hot-vectors for each batch**\n",
    "- or encode words of a sequence in one vector but that is not so nice since the word ordering is lost\n",
    "- _Target encoding_ - but that is probably not so easy to adapt to this problem since we need to classify multiple sentences and not single words & we would give away which sentences are important for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "z82N8ky0C3Y5",
   "metadata": {
    "id": "z82N8ky0C3Y5"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa4700e",
   "metadata": {
    "id": "dfa4700e"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels)).shuffle(60000).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9u9-O9rofZBU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9u9-O9rofZBU",
    "outputId": "e8d0bc8e-0426-42f2-a6ff-061a0d1c96f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0], shape=(64,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "  print(y)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "XWHX26Q3W8eL",
   "metadata": {
    "id": "XWHX26Q3W8eL"
   },
   "outputs": [],
   "source": [
    "sequence_lengths = [len(sequence) for sequence in test_sequences]\n",
    "max_len = max(sequence_lengths)\n",
    "\n",
    "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_sequences,\n",
    "    maxlen=max_len,\n",
    "    padding=\"pre\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels)).shuffle(60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QMFrfclfDoAS",
   "metadata": {
    "id": "QMFrfclfDoAS"
   },
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19eabfe7",
   "metadata": {
    "id": "19eabfe7"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  def __init__(self, num_features, num_outputs, num_hidden_units, rand_init_low_high=0.1, activation=tf.nn.sigmoid):\n",
    "    self.nhidden = num_hidden_units\n",
    "    self.activation = activation\n",
    "    self.num_words = num_features\n",
    "    self.Wh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_hidden_units)).astype(np.float32))\n",
    "    self.bh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
    "    self.Wx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_features, num_hidden_units)).astype(np.float32))\n",
    "    self.bx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
    "    self.Wy = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_outputs)).astype(np.float32))\n",
    "    self.by = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_outputs)).astype(np.float32))\n",
    "\n",
    "  def cell(self, xt, ht_1):\n",
    "    \"\"\"\n",
    "    xt: input x at timestep t\n",
    "    ht_1: output from hidden unit in previous timestep t-1\n",
    "\n",
    "    returns: yt, ht\n",
    "    \"\"\"\n",
    "    xo = tf.matmul(xt, self.Wx) + self.bx\n",
    "    ho = tf.matmul(ht_1, self.Wh) + self.bh\n",
    "    logits = xo + ho\n",
    "    ht = self.activation(logits)\n",
    "    #yt = tf.matmul(self.Wy, ht) + self.by\n",
    "    return ht\n",
    "\n",
    "  def compute_y(self, ht):\n",
    "    return tf.matmul(ht, self.Wy) + self.by\n",
    "\n",
    "  def __call__(self, x):\n",
    "    \"\"\"\n",
    "    batch is of shape (batch_size, timesteps, features)\n",
    "    returns: the logits in the last timestep\n",
    "    \"\"\"\n",
    "    # first hidden activation is 0\n",
    "    ht = tf.constant(np.zeros(shape=(x.shape[0], self.nhidden)).astype(np.float32))\n",
    "    one_hot_vectors = tf.one_hot(indices=tf.experimental.numpy.swapaxes(x,0,1), depth=self.num_words) # swap batch & timestep to get (timesteps, batch_size, features)\n",
    "    for xt in one_hot_vectors:\n",
    "      # only if not padded\n",
    "      if tf.reduce_max(xt[:,0]) != 0:\n",
    "        ht = self.cell(xt, ht)\n",
    "\n",
    "    return self.compute_y(ht)\n",
    "\n",
    "  def train(self, train_data, num_epochs, steps_per_epoch, batch_size, optimizer=tf.optimizers.Adam(), loss_fn=tf.losses.CategoricalCrossentropy(from_logits=True)):\n",
    "    variables = [self.Wh, self.Wx, self.Wy, self.bh, self.bx, self.by]\n",
    "    optimizer.build(variables)\n",
    "    for epoch in range(num_epochs):\n",
    "      losses = []\n",
    "      for i,(x,y) in enumerate(train_data):\n",
    "        if i >= steps_per_epoch:\n",
    "          break\n",
    "        with tf.GradientTape() as tape:\n",
    "          logits = self(x)\n",
    "          # shape of logits is (BATCH_SIZE,1) -> needs reshape\n",
    "          loss = loss_fn(y, tf.reshape(logits, (batch_size,)))\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "      print(\"Epoch {} done: loss = {}\".format(epoch, np.mean(losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "u7NVSiWnWVn3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7NVSiWnWVn3",
    "outputId": "0c15dfe6-b9aa-491b-fd34-d705fd10eced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done: loss = 130.5621337890625\n",
      "Epoch 1 done: loss = 124.7631607055664\n",
      "Epoch 2 done: loss = 143.0655517578125\n",
      "Epoch 3 done: loss = 141.4359893798828\n",
      "Epoch 4 done: loss = 138.10482788085938\n",
      "Epoch 5 done: loss = 127.27082824707031\n",
      "Epoch 6 done: loss = 123.10667419433594\n",
      "Epoch 7 done: loss = 126.40826416015625\n",
      "Epoch 8 done: loss = 129.7408447265625\n",
      "Epoch 9 done: loss = 129.75808715820312\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(num_words, 1, 100)\n",
    "rnn.train(train_data, 10, 5, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "_9AuEOLmXi5h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9AuEOLmXi5h",
    "outputId": "79e1c4fd-f20e-4373-cc4f-060d9f7a4aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47999999999999987"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 100\n",
    "\n",
    "accuracy = 0\n",
    "for n, (x,y) in enumerate(test_data.batch(1)):\n",
    "  if n >= TEST_SIZE:\n",
    "    break\n",
    "\n",
    "  guess = rnn(x)\n",
    "  if (guess > 0.5 and y == 1) or (guess <= 0.5 and y == 0):\n",
    "    accuracy = (n)/(n+1) * accuracy + 1/(n+1)\n",
    "  else:\n",
    "    accuracy = (n)/(n+1) * accuracy\n",
    "\n",
    "  print(\"\\r\" + str(accuracy), end='', flush=True)\n",
    "\n",
    "\n",
    "# E_n = 1/n * sum(1,n,x_i)\n",
    "#     = 1/n (sum(1,n-1,x_i)+x_n)\n",
    "#     = (n-1)/n * 1/(n-1) * sum(1,n-1,x_i) + x_n/n\n",
    "#     = (n-1)/n * E_n-1 + x_n/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F7WmXeWuq1lP",
   "metadata": {
    "id": "F7WmXeWuq1lP"
   },
   "source": [
    "**Observations**\n",
    "- the forward step is really slow\n",
    "- therefore training this takes ages\n",
    "  - _**How can you speed this up or is it correct because of the sequential manner of the RNN?**_\n",
    "- _currently_: network is guessing (accuracy of around 50%)\n",
    "\n",
    "**Thoughts about outputs**\n",
    "- having one output means that e.g. 1 is hate speech and 0 is not\n",
    "  - therefore, the relation between them is hate_speech = 1 - friendly\n",
    "- having 2 output units on the other hand means that output unit 1 is for hate and output unit 2 is for friendly\n",
    "  - here they don't necessarly need to add up to 1 since a text may criticise but also emphasises good parts of the movie\n",
    "- HERE: it is easier with one unit because you can directly compare with the given binary labels -> _there is no such intermediate thing as mentioned above_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_HsjRtprseqS",
   "metadata": {
    "id": "_HsjRtprseqS"
   },
   "source": [
    "## Open problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eFU0UlZFsi6R",
   "metadata": {
    "id": "eFU0UlZFsi6R"
   },
   "source": [
    "**initial state**\n",
    "- in this case there is no need to learn an initial state because each review is independent of the other ones\n",
    "- the initial state would give a tendence if a review is positiv or negative and this is not feasible here\n",
    "\n",
    "**when to pad**\n",
    "- pre-padding would probably be better, because when the sentence ends we generate the output instead of having to feed the current hidden activation through he network\n",
    "\n",
    "**avoid computing padded sequences**\n",
    "- Since we padded with 0, we could check if hot_vector[0] contains a 1\n",
    "- if it contains it, we could skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2ITw5FTxN6Y",
   "metadata": {
    "id": "k2ITw5FTxN6Y"
   },
   "source": [
    "## Use outputs from every timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjnHIsHgzF5q",
   "metadata": {
    "id": "jjnHIsHgzF5q"
   },
   "source": [
    "**incorporating all outputs**\n",
    "- averaging logits\n",
    "- averaging hidden states\n",
    "  - if logits means the logits before the sigmoid activation to get the next hidden state, then _states_ and logits are the same\n",
    "  - if logits means the logits before the activation when computing the output y, then it is different\n",
    "    - here: the logits would have gone through one more matrix multiplication and there are less logits than in the hidden states\n",
    "- averaging sigmoids\n",
    "  - this means that we average how sure the network was during reading the text from beginning to end\n",
    "  - this is similar to a human reading it and trying to understand if it is positive or negative\n",
    "  - but averaging this - especially per word - can be bad because many words on their own aren't important for the meaning\n",
    "  - is different because we average wth the non-linearities\n",
    "  - especially with sigmoid is it bad, because really large logits map to nearly the same value as quite small logits -> many smaller logits can out weight the really large one where the network was really sure\n",
    "\n",
    "**advantages of such techniques**\n",
    "- when the model thinks really hard at the beginning that this is a negative / positive text, it needs to propagate this through the whole network\n",
    "- but with these averaging techniques there are some kind of skip connections to the end\n",
    "- this improves gradient flow and makes it easier to propagate information through the network\n",
    "\n",
    "**disadvantages**\n",
    "- makes it a little bit more complicated\n",
    "- can be problematic because the network has more options and can behave worse in some cases\n",
    "\n",
    "> **_use averaging logits (before y)_** because it keeps quite a lot of information and Wy is used more often and can be trained better instead of having just 1 operation with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hkejM0zZxT2d",
   "metadata": {
    "id": "hkejM0zZxT2d"
   },
   "outputs": [],
   "source": [
    "class RNN2:\n",
    "  def __init__(self, num_features, num_outputs, num_hidden_units, rand_init_low_high=0.1, activation=tf.nn.sigmoid):\n",
    "    self.nhidden = num_hidden_units\n",
    "    self.activation = activation\n",
    "    self.num_words = num_features\n",
    "    self.ys = []\n",
    "    self.Wh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_hidden_units)).astype(np.float32))\n",
    "    self.bh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
    "    self.Wx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_features, num_hidden_units)).astype(np.float32))\n",
    "    self.bx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
    "    self.Wy = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_outputs)).astype(np.float32))\n",
    "    self.by = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_outputs)).astype(np.float32))\n",
    "\n",
    "  def cell(self, xt, ht_1):\n",
    "    \"\"\"\n",
    "    xt: input x at timestep t\n",
    "    ht_1: output from hidden unit in previous timestep t-1\n",
    "\n",
    "    returns: yt, ht\n",
    "    \"\"\"\n",
    "    xo = tf.matmul(xt, self.Wx) + self.bx\n",
    "    ho = tf.matmul(ht_1, self.Wh) + self.bh\n",
    "    logits = xo + ho\n",
    "    ht = self.activation(logits)\n",
    "    # keep track of logits at each timestep\n",
    "    self.ys.append(self.compute_y(ht))\n",
    "    return ht\n",
    "\n",
    "  def compute_y(self, ht):\n",
    "    return tf.matmul(ht, self.Wy) + self.by\n",
    "\n",
    "  def average_output_guess(self):\n",
    "    if len(self.ys) <= 0:\n",
    "      raise AssertionError(\"There was no output (logit) recorded.\")\n",
    "    # reduce along the timestep axis -> keeping batches\n",
    "    return tf.nn.sigmoid(tf.reduce_mean(self.ys, axis=0))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    \"\"\"\n",
    "    batch is of shape (batch_size, timesteps, features)\n",
    "    returns: the logits in the last timestep\n",
    "    \"\"\"\n",
    "    # initialize logits tracking\n",
    "    self.ys = []\n",
    "    # first hidden activation is 0\n",
    "    ht = tf.Variable(np.zeros(shape=(x.shape[0], self.nhidden)).astype(np.float32))\n",
    "    one_hot_vectors = tf.one_hot(indices=np.swapaxes(x,0,1), depth=self.num_words) # swap batch & timestep to get (timesteps, batch_size, features)\n",
    "    for xt in one_hot_vectors:\n",
    "      # only if not padded\n",
    "      if tf.reduce_max(xt[:,0]) != 0:\n",
    "        ht = self.cell(xt, ht)\n",
    "\n",
    "    return self.average_output_guess()\n",
    "\n",
    "  def train(self, train_data, num_epochs, steps_per_epoch, optimizer=tf.optimizers.Adam(), loss_fn=tf.losses.CategoricalCrossentropy(from_logits=True)):\n",
    "    variables = [self.Wh, self.Wx, self.Wy, self.bh, self.bx, self.by]\n",
    "    optimizer.build(variables)\n",
    "    for epoch in range(num_epochs):\n",
    "      losses = []\n",
    "      for i,(x,y) in enumerate(train_data):\n",
    "        if i >= steps_per_epoch:\n",
    "          break\n",
    "        with tf.GradientTape() as tape:\n",
    "          logits = self(x)\n",
    "          loss = loss_fn(y, tf.reshape(logits, (x.shape[0],)))\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "      print(\"Epoch {} done: loss = {}\".format(epoch, np.mean(losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ficdmZ5t-Med",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ficdmZ5t-Med",
    "outputId": "c932a1bc-b5ba-45ca-8564-f200b81531fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done: loss = 145.56103515625\n",
      "Epoch 1 done: loss = 145.5611114501953\n",
      "Epoch 2 done: loss = 112.28897857666016\n",
      "Epoch 3 done: loss = 149.7200927734375\n",
      "Epoch 4 done: loss = 124.76795959472656\n",
      "Epoch 5 done: loss = 112.2895278930664\n",
      "Epoch 6 done: loss = 158.0383758544922\n",
      "Epoch 7 done: loss = 158.03810119628906\n",
      "Epoch 8 done: loss = 124.76657104492188\n",
      "Epoch 9 done: loss = 95.65647888183594\n"
     ]
    }
   ],
   "source": [
    "rnn2 = RNN2(num_words, 1, 100)\n",
    "rnn2.train(train_data, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dkf-i_qINwQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dkf-i_qINwQ",
    "outputId": "468c98fb-823c-4df4-d44d-70fb9133791f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99: accuracy = 0.5299999999999998"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for n, (x,y) in enumerate(test_data.batch(1)):\n",
    "  if n >= TEST_SIZE:\n",
    "    break\n",
    "\n",
    "  guess = rnn2(x)\n",
    "  if (guess > 0.5 and y == 1) or (guess <= 0.5 and y == 0):\n",
    "    accuracy = (n)/(n+1) * accuracy + 1/(n+1)\n",
    "  else:\n",
    "    accuracy = (n)/(n+1) * accuracy\n",
    "\n",
    "  print(\"\\rStep {}: accuracy = {}\".format(n,accuracy), end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HpV43ua4KNMz",
   "metadata": {
    "id": "HpV43ua4KNMz"
   },
   "source": [
    "**Conclusion**\n",
    "- using all intermediate outputs and averaging them seems to work a little bit better than only taking the last one\n",
    "- therefore, _**training now intensively**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "UwG0bB-rKdAE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwG0bB-rKdAE",
    "outputId": "8bb748bd-739e-473c-ec72-70b7f7ae6395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done: loss = 134.2486572265625\n",
      "Epoch 1 done: loss = 133.5832977294922\n",
      "Epoch 2 done: loss = 134.74778747558594\n",
      "Epoch 3 done: loss = 132.50201416015625\n",
      "Epoch 4 done: loss = 133.91603088378906\n",
      "Epoch 5 done: loss = 135.0805206298828\n",
      "Epoch 6 done: loss = 134.58145141601562\n",
      "Epoch 7 done: loss = 131.91976928710938\n",
      "Epoch 8 done: loss = 134.49827575683594\n",
      "Epoch 9 done: loss = 131.75341796875\n",
      "Epoch 10 done: loss = 134.8309783935547\n",
      "Epoch 11 done: loss = 136.32818603515625\n",
      "Epoch 12 done: loss = 134.49827575683594\n",
      "Epoch 13 done: loss = 135.1636962890625\n",
      "Epoch 14 done: loss = 133.91604614257812\n",
      "Epoch 15 done: loss = 132.3356475830078\n",
      "Epoch 16 done: loss = 129.008544921875\n",
      "Epoch 17 done: loss = 132.2524871826172\n",
      "Epoch 18 done: loss = 140.15435791015625\n",
      "Epoch 19 done: loss = 133.91603088378906\n"
     ]
    }
   ],
   "source": [
    "rnn2.train(train_data, 20, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768b11d",
   "metadata": {},
   "source": [
    "## Further possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a9168",
   "metadata": {},
   "source": [
    "We had the idea that adjectives and adverbs are most important for classifying a text as positive or negative.\n",
    "Therefore, we tried some preprocessing which shrinks the vocabulary size and wouldspeed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a362eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "\n",
    "from lemminflect import getAllLemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
    "num_words = 5000\n",
    "\n",
    "# Use the default parameters to keras.datasets.imdb.load_data\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "\n",
    "# Retrieve the dataset\n",
    "(train_seqs, train_labels), (test_seqs, test_labels) = keras.datasets.imdb.load_data(start_char=start_char, oov_char=oov_char, index_from=index_from, num_words=num_words)\n",
    "\n",
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = dict(sorted(keras.datasets.imdb.get_word_index().items()))\n",
    "word_index[\"[START]\"] = start_char\n",
    "word_index[\"[OOV]\"] = oov_char\n",
    "\n",
    "print(f\"First 100 elements of the word_index: {list(word_index.items())[:100]}\")\n",
    "\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict((i + index_from, word) for (word, i) in word_index.items())\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "\n",
    "inverted_word_index = dict(sorted(inverted_word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75306675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indx_to_word(i: int) -> str:\n",
    "  return inverted_word_index.get(i, \"UNKNOWN\")\n",
    "\n",
    "def word_to_indx(w: str) -> int:\n",
    "  return word_index[w]+3\n",
    "\n",
    "def seq_to_words(seq: list[str]) -> str:\n",
    "  \"\"\"\n",
    "  Converts a sequence (a list of indices, decoding words) to decoded string (a list of strings the indices decode)\n",
    "  \"\"\"\n",
    "  return [indx_to_word(indx) for indx in seq]\n",
    "\n",
    "def seq_to_text(seq: list[str]) -> str:\n",
    "  \"\"\"\n",
    "  Converts a sequence (a list of indices, decoding words) to decoded human readable text (one large string)\n",
    "  \"\"\"\n",
    "  return \" \".join(seq_to_words(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fe1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK library has a really bad performance for detecting adjectives ~ 50%\n",
    "def is_adjective_or_adverb_using_nltk(s: str) -> bool:\n",
    "  nltk_type_of_word = nltk.pos_tag([s], tagset=\"universal\")[0][1]\n",
    "  is_adjective_or_adverb = nltk_type_of_word == \"ADJ\" or nltk_type_of_word == \"ADV\"\n",
    "  #if is_adjective_or_adverb:\n",
    "  print(f\"{s} {'is an adjective or adverb' if is_adjective_or_adverb else f'is not an adjective or adverb, it is: {nltk_type_of_word}'}\")\n",
    "  return is_adjective_or_adverb\n",
    "\n",
    "# Lemminflect's performance is > 90%, way better!\n",
    "def is_adjective_or_adverb(s: str) -> bool:\n",
    "  lemmas = getAllLemmas(s)\n",
    "  #print(f\"{s}'s lemmas: {lemmas}\")\n",
    "  return \"ADJ\" in lemmas or \"ADV\" in lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a534ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process a single string\n",
    "def pre_process_str(s: str) -> str:\n",
    "\n",
    "  # Remove punctuation\n",
    "  def remove_punctuation(s: str) -> str:\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    return s\n",
    "\n",
    "  return remove_punctuation(s)\n",
    "\n",
    "\n",
    "# Pre-process an entire sequence\n",
    "def pre_process_sequence(seq: list[int]) -> list[int]:\n",
    "  words = seq_to_words(seq)\n",
    "  words_processed = [w for w in words if(w != \"UNKNOWN\" and  w != \"[OOV]\" and w != \"[START]\" and is_adjective_or_adverb(w))]\n",
    "  return [word_to_indx(w) for w in words_processed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f309fe90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W83Ynq0sZ3to",
    "outputId": "6929ac97-9541-4691-f864-07e402fa3891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without processing: [START] this film was just brilliant casting location scenery story direction everyone's really suited [START] part they played [OOV] you could just imagine being there robert [OOV] is an amazing actor [OOV] now [START] same being director [OOV] father came from [START] same scottish island as myself so i loved [START] fact there was a real connection with this film [START] witty remarks throughout [START] film were great it was just brilliant so much that i bought [START] film as soon as it was released for [OOV] [OOV] would recommend it to everyone to watch [OOV] [START] fly [OOV] was amazing really cried at [START] end it was so sad [OOV] you know what they say if you cry at a film it must have been good [OOV] this definitely was also [OOV] to [START] two little [OOV] that played [START] [OOV] of norman [OOV] paul they were just brilliant children are often left out of [START] [OOV] list i think because [START] stars that play them all grown up are such a big [OOV] for [START] whole film but these children are amazing [OOV] should be [OOV] for what they have done don't you think [START] whole story was so lovely because it was true [OOV] was someone's life after all that was [OOV] with us all\n",
      "After preprocessing: just brilliant really part just being there amazing now same being same as so there real witty throughout great just brilliant so much as soon as to to amazing really so sad good definitely also to two little just brilliant often left out up big whole amazing done whole so lovely true after\n"
     ]
    }
   ],
   "source": [
    "print(f\"Without processing: {seq_to_text(train_seqs[0])}\")\n",
    "print(f\"After preprocessing: {seq_to_text(pre_process_sequence(train_seqs[0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87587a",
   "metadata": {},
   "source": [
    "Unfortunatly we had _**no time to try it out**_ with the model because doing research for this was really time intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75be31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4126b186",
    "bada4d4e",
    "QMFrfclfDoAS",
    "_HsjRtprseqS"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
