{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9aea25c1",
      "metadata": {
        "id": "9aea25c1"
      },
      "source": [
        "# [Assignment 5](https://ovgu-ailab.github.io/idl2023/assignment5.html)\n",
        "\n",
        "Adrian Bremer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4126b186",
      "metadata": {
        "id": "4126b186"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4dc6a38e",
      "metadata": {
        "id": "4dc6a38e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bada4d4e",
      "metadata": {
        "id": "bada4d4e"
      },
      "source": [
        "## Preparing IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "80386305",
      "metadata": {
        "id": "80386305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0649d1a5-97d3-4fce-8ab0-46bbac3e9b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "num_words = 20000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Ef172nfM80",
        "outputId": "79bfe888-56a2-4f72-9f90-3388e16e5023"
      },
      "id": "j7Ef172nfM80",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "599876ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599876ed",
        "outputId": "4c143582-bd3c-4571-bb05-051601976b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2494\n",
            "238\n"
          ]
        }
      ],
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "\n",
        "max_len = max(sequence_lengths)\n",
        "print(max_len)\n",
        "mean_len = int(np.mean(sequence_lengths))\n",
        "print(mean_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ideas for using not the full-length padding scheme**\n",
        "- use the mean length and every other word is _UNKNOWN_\n",
        "  - _truncating_ instead of throwing away since the long sequences are important too because when they are longer the way it is written is different & truncating _the back_ (post) because mostly the first few words are like \"Ehh, this is bad\""
      ],
      "metadata": {
        "id": "0bnYsSd3kZ22"
      },
      "id": "0bnYsSd3kZ22"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "01ecf2cd",
      "metadata": {
        "id": "01ecf2cd"
      },
      "outputs": [],
      "source": [
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen=mean_len, # max_len\n",
        "    padding=\"pre\",\n",
        "    truncating=\"post\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0ibtLQDZ7V",
        "outputId": "ad0e369a-92d1-40d1-a49d-b471f998f4e4"
      },
      "id": "Bt0ibtLQDZ7V",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 238)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c359f1ec",
      "metadata": {
        "id": "c359f1ec"
      },
      "outputs": [],
      "source": [
        "# one_hot_sequences_padded = tf.one_hot(indices=train_sequences_padded, depth=num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "- the one-hot-vectors are too large so RAM isn't enough\n",
        "\n",
        "**Ideas to fix this**\n",
        "- for storage use integers as vectors because we only need 1 or 0\n",
        "  - here the indices could be used right away\n",
        "- or just use the indices and **construct the one-hot-vectors for each batch**\n",
        "- or encode words of a sequence in one vector but that is not so nice since the word ordering is lost\n",
        "- _Target encoding_ - but that is probably not so easy to adapt to this problem since we need to classify multiple sentences and not single words & we would give away which sentences are important for the classification"
      ],
      "metadata": {
        "id": "MFdqkgWng7_8"
      },
      "id": "MFdqkgWng7_8"
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "z82N8ky0C3Y5"
      },
      "id": "z82N8ky0C3Y5",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dfa4700e",
      "metadata": {
        "id": "dfa4700e"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels)).shuffle(60000).repeat().batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_data:\n",
        "  print(y)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u9-O9rofZBU",
        "outputId": "e8d0bc8e-0426-42f2-a6ff-061a0d1c96f0"
      },
      "id": "9u9-O9rofZBU",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0], shape=(64,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_lengths = [len(sequence) for sequence in test_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "\n",
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_sequences,\n",
        "    maxlen=max_len,\n",
        "    padding=\"pre\",\n",
        "    truncating=\"post\"\n",
        ")\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels)).shuffle(60000)"
      ],
      "metadata": {
        "id": "XWHX26Q3W8eL"
      },
      "id": "XWHX26Q3W8eL",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the RNN"
      ],
      "metadata": {
        "id": "QMFrfclfDoAS"
      },
      "id": "QMFrfclfDoAS"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "19eabfe7",
      "metadata": {
        "id": "19eabfe7"
      },
      "outputs": [],
      "source": [
        "class RNN:\n",
        "  def __init__(self, num_features, num_outputs, num_hidden_units, rand_init_low_high=0.1, activation=tf.nn.sigmoid):\n",
        "    self.nhidden = num_hidden_units\n",
        "    self.activation = activation\n",
        "    self.num_words = num_features\n",
        "    self.Wh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_hidden_units)).astype(np.float32))\n",
        "    self.bh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
        "    self.Wx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_features, num_hidden_units)).astype(np.float32))\n",
        "    self.bx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
        "    self.Wy = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_outputs)).astype(np.float32))\n",
        "    self.by = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_outputs)).astype(np.float32))\n",
        "\n",
        "  def cell(self, xt, ht_1):\n",
        "    \"\"\"\n",
        "    xt: input x at timestep t\n",
        "    ht_1: output from hidden unit in previous timestep t-1\n",
        "\n",
        "    returns: yt, ht\n",
        "    \"\"\"\n",
        "    xo = tf.matmul(xt, self.Wx) + self.bx\n",
        "    ho = tf.matmul(ht_1, self.Wh) + self.bh\n",
        "    logits = xo + ho\n",
        "    ht = self.activation(logits)\n",
        "    #yt = tf.matmul(self.Wy, ht) + self.by\n",
        "    return ht\n",
        "\n",
        "  def compute_y(self, ht):\n",
        "    return tf.matmul(ht, self.Wy) + self.by\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"\n",
        "    batch is of shape (batch_size, timesteps, features)\n",
        "    returns: the logits in the last timestep\n",
        "    \"\"\"\n",
        "    # first hidden activation is 0\n",
        "    ht = tf.constant(np.zeros(shape=(x.shape[0], self.nhidden)).astype(np.float32))\n",
        "    one_hot_vectors = tf.one_hot(indices=tf.experimental.numpy.swapaxes(x,0,1), depth=self.num_words) # swap batch & timestep to get (timesteps, batch_size, features)\n",
        "    for xt in one_hot_vectors:\n",
        "      # only if not padded\n",
        "      if tf.reduce_max(xt[:,0]) != 0:\n",
        "        ht = self.cell(xt, ht)\n",
        "      else:\n",
        "        break # after first padding only paddign will follow\n",
        "\n",
        "    return self.compute_y(ht)\n",
        "\n",
        "  def train(self, train_data, num_epochs, steps_per_epoch, batch_size, optimizer=tf.optimizers.Adam(), loss_fn=tf.losses.CategoricalCrossentropy(from_logits=True)):\n",
        "    variables = [self.Wh, self.Wx, self.Wy, self.bh, self.bx, self.by]\n",
        "    optimizer.build(variables)\n",
        "    for epoch in range(num_epochs):\n",
        "      losses = []\n",
        "      for i,(x,y) in enumerate(train_data):\n",
        "        if i >= steps_per_epoch:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:\n",
        "          logits = self(x)\n",
        "          loss = loss_fn(y, tf.reshape(logits, (batch_size,)))\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "      print(\"Epoch {} done: loss = {}\".format(epoch, np.mean(losses)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = RNN(num_words, 1, 100)\n",
        "rnn.train(train_data, 10, 5, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7NVSiWnWVn3",
        "outputId": "0c15dfe6-b9aa-491b-fd34-d705fd10eced"
      },
      "id": "u7NVSiWnWVn3",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 done: loss = 130.5621337890625\n",
            "Epoch 1 done: loss = 124.7631607055664\n",
            "Epoch 2 done: loss = 143.0655517578125\n",
            "Epoch 3 done: loss = 141.4359893798828\n",
            "Epoch 4 done: loss = 138.10482788085938\n",
            "Epoch 5 done: loss = 127.27082824707031\n",
            "Epoch 6 done: loss = 123.10667419433594\n",
            "Epoch 7 done: loss = 126.40826416015625\n",
            "Epoch 8 done: loss = 129.7408447265625\n",
            "Epoch 9 done: loss = 129.75808715820312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = 100\n",
        "\n",
        "accuracy = 0\n",
        "for n, (x,y) in enumerate(test_data.batch(1)):\n",
        "  if n >= TEST_SIZE:\n",
        "    break\n",
        "\n",
        "  guess = rnn(x)\n",
        "  if (guess > 0.5 and y == 1) or (guess <= 0.5 and y == 0):\n",
        "    accuracy = (n)/(n+1) * accuracy + 1/(n+1)\n",
        "  else:\n",
        "    accuracy = (n)/(n+1) * accuracy\n",
        "\n",
        "  print(\"\\r\" + str(accuracy), end='', flush=True)\n",
        "\n",
        "\n",
        "# E_n = 1/n * sum(1,n,x_i)\n",
        "#     = 1/n (sum(1,n-1,x_i)+x_n)\n",
        "#     = (n-1)/n * 1/(n-1) * sum(1,n-1,x_i) + x_n/n\n",
        "#     = (n-1)/n * E_n-1 + x_n/n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9AuEOLmXi5h",
        "outputId": "79e1c4fd-f20e-4373-cc4f-060d9f7a4aa3"
      },
      "id": "_9AuEOLmXi5h",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47999999999999987"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "- the forward step is really slow\n",
        "- therefore training this takes ages\n",
        "  - _**How can you speed this up or is it correct because of the sequential manner of the RNN?**_\n",
        "- _currently_: network is guessing (accuracy of around 50%)\n",
        "\n",
        "**Thoughts about outputs**\n",
        "- having one output means that e.g. 1 is hate speech and 0 is not\n",
        "  - therefore, the relation between them is hate_speech = 1 - friendly\n",
        "- having 2 output units on the other hand means that output unit 1 is for hate and output unit 2 is for friendly\n",
        "  - here they don't necessarly need to add up to 1 since a text may criticise but also emphasises good parts of the movie\n",
        "- HERE: it is easier with one unit because you can directly compare with the given binary labels -> _there is no such intermediate thing as mentioned above_\n"
      ],
      "metadata": {
        "id": "F7WmXeWuq1lP"
      },
      "id": "F7WmXeWuq1lP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open problems"
      ],
      "metadata": {
        "id": "_HsjRtprseqS"
      },
      "id": "_HsjRtprseqS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**initial state**\n",
        "- in this case there is no need to learn an initial state because each review is independent of the other ones\n",
        "- the initial state would give a tendence if a review is positiv or negative and this is not feasible here\n",
        "\n",
        "**when to pad**\n",
        "- pre-padding would probably be better, because when the sentence ends we generate the output instead of having to feed the current hidden activation through he network\n",
        "\n",
        "**avoid computing padded sequences**\n",
        "- Since we padded with 0, we could check if hot_vector[0] contains a 1\n",
        "- if it contains it, we could skip"
      ],
      "metadata": {
        "id": "eFU0UlZFsi6R"
      },
      "id": "eFU0UlZFsi6R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use outputs from every timestep"
      ],
      "metadata": {
        "id": "k2ITw5FTxN6Y"
      },
      "id": "k2ITw5FTxN6Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**incorporating all outputs**\n",
        "- averaging logits\n",
        "- averaging hidden states\n",
        "  - if logits means the logits before the sigmoid activation to get the next hidden state, then _states_ and logits are the same\n",
        "  - if logits means the logits before the activation when computing the output y, then it is different\n",
        "    - here: the logits would have gone through one more matrix multiplication and there are less logits than in the hidden states\n",
        "- averaging sigmoids\n",
        "  - this means that we average how sure the network was during reading the text from beginning to end\n",
        "  - this is similar to a human reading it and trying to understand if it is positive or negative\n",
        "  - but averaging this - especially per word - can be bad because many words on their own aren't important for the meaning\n",
        "  - is different because we average wth the non-linearities\n",
        "  - especially with sigmoid is it bad, because really large logits map to nearly the same value as quite small logits -> many smaller logits can out weight the really large one where the network was really sure\n",
        "\n",
        "**advantages of such techniques**\n",
        "- when the model thinks really hard at the beginning that this is a negative / positive text, it needs to propagate this through the whole network\n",
        "- but with these averaging techniques there are some kind of skip connections to the end\n",
        "- this improves gradient flow and makes it easier to propagate information through the network\n",
        "\n",
        "**disadvantages**\n",
        "- makes it a little bit more complicated\n",
        "- can be problematic because the network has more options and can behave worse in some cases\n",
        "\n",
        "> **_use averaging logits (before y)_** because it keeps quite a lot of information and Wy is used more often and can be trained better instead of having just 1 operation with it"
      ],
      "metadata": {
        "id": "jjnHIsHgzF5q"
      },
      "id": "jjnHIsHgzF5q"
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN2:\n",
        "  def __init__(self, num_features, num_outputs, num_hidden_units, rand_init_low_high=0.1, activation=tf.nn.sigmoid):\n",
        "    self.nhidden = num_hidden_units\n",
        "    self.activation = activation\n",
        "    self.num_words = num_features\n",
        "    self.ys = []\n",
        "    self.Wh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_hidden_units)).astype(np.float32))\n",
        "    self.bh = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
        "    self.Wx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_features, num_hidden_units)).astype(np.float32))\n",
        "    self.bx = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units)).astype(np.float32))\n",
        "    self.Wy = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_hidden_units, num_outputs)).astype(np.float32))\n",
        "    self.by = tf.Variable(np.random.uniform(low=-rand_init_low_high, high=rand_init_low_high, size=(num_outputs)).astype(np.float32))\n",
        "\n",
        "  def cell(self, xt, ht_1):\n",
        "    \"\"\"\n",
        "    xt: input x at timestep t\n",
        "    ht_1: output from hidden unit in previous timestep t-1\n",
        "\n",
        "    returns: yt, ht\n",
        "    \"\"\"\n",
        "    xo = tf.matmul(xt, self.Wx) + self.bx\n",
        "    ho = tf.matmul(ht_1, self.Wh) + self.bh\n",
        "    logits = xo + ho\n",
        "    ht = self.activation(logits)\n",
        "    # keep track of logits at each timestep\n",
        "    self.ys.append(self.compute_y(ht))\n",
        "    return ht\n",
        "\n",
        "  def compute_y(self, ht):\n",
        "    return tf.matmul(ht, self.Wy) + self.by\n",
        "\n",
        "  def average_output_guess(self):\n",
        "    if len(self.ys) <= 0:\n",
        "      raise AssertionError(\"There was no output (logit) recorded.\")\n",
        "    # reduce along the timestep axis -> keeping batches\n",
        "    return tf.nn.sigmoid(tf.reduce_mean(self.ys, axis=0))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"\n",
        "    batch is of shape (batch_size, timesteps, features)\n",
        "    returns: the logits in the last timestep\n",
        "    \"\"\"\n",
        "    # initialize logits tracking\n",
        "    self.ys = []\n",
        "    # first hidden activation is 0\n",
        "    ht = tf.Variable(np.zeros(shape=(x.shape[0], self.nhidden)).astype(np.float32))\n",
        "    one_hot_vectors = tf.one_hot(indices=np.swapaxes(x,0,1), depth=self.num_words) # swap batch & timestep to get (timesteps, batch_size, features)\n",
        "    for xt in one_hot_vectors:\n",
        "      # only if not padded\n",
        "      if tf.reduce_max(xt[:,0]) != 0:\n",
        "        ht = self.cell(xt, ht)\n",
        "\n",
        "    return self.average_output_guess()\n",
        "\n",
        "  def train(self, train_data, num_epochs, steps_per_epoch, optimizer=tf.optimizers.Adam(), loss_fn=tf.losses.CategoricalCrossentropy(from_logits=True)):\n",
        "    variables = [self.Wh, self.Wx, self.Wy, self.bh, self.bx, self.by]\n",
        "    optimizer.build(variables)\n",
        "    for epoch in range(num_epochs):\n",
        "      losses = []\n",
        "      for i,(x,y) in enumerate(train_data):\n",
        "        if i >= steps_per_epoch:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:\n",
        "          logits = self(x)\n",
        "          loss = loss_fn(y, tf.reshape(logits, (x.shape[0],)))\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "      print(\"Epoch {} done: loss = {}\".format(epoch, np.mean(losses)))\n"
      ],
      "metadata": {
        "id": "hkejM0zZxT2d"
      },
      "id": "hkejM0zZxT2d",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn2 = RNN2(num_words, 1, 100)\n",
        "rnn2.train(train_data, 10, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ficdmZ5t-Med",
        "outputId": "c932a1bc-b5ba-45ca-8564-f200b81531fd"
      },
      "id": "ficdmZ5t-Med",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 done: loss = 145.56103515625\n",
            "Epoch 1 done: loss = 145.5611114501953\n",
            "Epoch 2 done: loss = 112.28897857666016\n",
            "Epoch 3 done: loss = 149.7200927734375\n",
            "Epoch 4 done: loss = 124.76795959472656\n",
            "Epoch 5 done: loss = 112.2895278930664\n",
            "Epoch 6 done: loss = 158.0383758544922\n",
            "Epoch 7 done: loss = 158.03810119628906\n",
            "Epoch 8 done: loss = 124.76657104492188\n",
            "Epoch 9 done: loss = 95.65647888183594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = 0\n",
        "for n, (x,y) in enumerate(test_data.batch(1)):\n",
        "  if n >= TEST_SIZE:\n",
        "    break\n",
        "\n",
        "  guess = rnn2(x)\n",
        "  if (guess > 0.5 and y == 1) or (guess <= 0.5 and y == 0):\n",
        "    accuracy = (n)/(n+1) * accuracy + 1/(n+1)\n",
        "  else:\n",
        "    accuracy = (n)/(n+1) * accuracy\n",
        "\n",
        "  print(\"\\rStep {}: accuracy = {}\".format(n,accuracy), end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dkf-i_qINwQ",
        "outputId": "468c98fb-823c-4df4-d44d-70fb9133791f"
      },
      "id": "5dkf-i_qINwQ",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 99: accuracy = 0.5299999999999998"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "- using all intermediate outputs and averaging them seems to work a little bit better than only taking the last one\n",
        "- therefore, _**training now intensively**_"
      ],
      "metadata": {
        "id": "HpV43ua4KNMz"
      },
      "id": "HpV43ua4KNMz"
    },
    {
      "cell_type": "code",
      "source": [
        "rnn2.train(train_data, 20, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwG0bB-rKdAE",
        "outputId": "8bb748bd-739e-473c-ec72-70b7f7ae6395"
      },
      "id": "UwG0bB-rKdAE",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 done: loss = 134.2486572265625\n",
            "Epoch 1 done: loss = 133.5832977294922\n",
            "Epoch 2 done: loss = 134.74778747558594\n",
            "Epoch 3 done: loss = 132.50201416015625\n",
            "Epoch 4 done: loss = 133.91603088378906\n",
            "Epoch 5 done: loss = 135.0805206298828\n",
            "Epoch 6 done: loss = 134.58145141601562\n",
            "Epoch 7 done: loss = 131.91976928710938\n",
            "Epoch 8 done: loss = 134.49827575683594\n",
            "Epoch 9 done: loss = 131.75341796875\n",
            "Epoch 10 done: loss = 134.8309783935547\n",
            "Epoch 11 done: loss = 136.32818603515625\n",
            "Epoch 12 done: loss = 134.49827575683594\n",
            "Epoch 13 done: loss = 135.1636962890625\n",
            "Epoch 14 done: loss = 133.91604614257812\n",
            "Epoch 15 done: loss = 132.3356475830078\n",
            "Epoch 16 done: loss = 129.008544921875\n",
            "Epoch 17 done: loss = 132.2524871826172\n",
            "Epoch 18 done: loss = 140.15435791015625\n",
            "Epoch 19 done: loss = 133.91603088378906\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4126b186",
        "bada4d4e",
        "QMFrfclfDoAS",
        "_HsjRtprseqS"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}