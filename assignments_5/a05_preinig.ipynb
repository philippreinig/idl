{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 5\n",
        "Collaborative work by Adrian Bremer & Philipp Reinig\n"
      ],
      "metadata": {
        "id": "DEEo4FAmPCR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "_BPhKu9OPIMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ysbGXRwNPJ6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lemminflect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCs5y9snH62H",
        "outputId": "009d9ac0-86aa-4ee2-a6fb-b5b66252ef09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lemminflect) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Chp8YyyvxN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "import statistics\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "\n",
        "from lemminflect import getAllLemmas"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF1ElJzXIAyI",
        "outputId": "034f6e95-4224-4e5a-aa5c-2a7aceb1ca1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and exploring the Dataset"
      ],
      "metadata": {
        "id": "JF-CCSCRPL07"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVE7aMvC4iXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9507b40e-93e1-4ab2-f68e-9770125e8aad"
      },
      "source": [
        "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
        "num_words = 5000\n",
        "\n",
        "# Use the default parameters to keras.datasets.imdb.load_data\n",
        "start_char = 1\n",
        "oov_char = 2\n",
        "index_from = 3\n",
        "\n",
        "# Retrieve the dataset\n",
        "(train_seqs, train_labels), (test_seqs, test_labels) = keras.datasets.imdb.load_data(start_char=start_char, oov_char=oov_char, index_from=index_from, num_words=num_words)\n",
        "\n",
        "# Retrieve the word index file mapping words to indices\n",
        "word_index = dict(sorted(keras.datasets.imdb.get_word_index().items()))\n",
        "word_index[\"[START]\"] = start_char\n",
        "word_index[\"[OOV]\"] = oov_char\n",
        "\n",
        "print(f\"First 100 elements of the word_index: {list(word_index.items())[:100]}\")\n",
        "\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "# And add `index_from` to indices to sync with `x_train`\n",
        "inverted_word_index = dict((i + index_from, word) for (word, i) in word_index.items())\n",
        "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
        "inverted_word_index[start_char] = \"[START]\"\n",
        "inverted_word_index[oov_char] = \"[OOV]\"\n",
        "\n",
        "inverted_word_index = dict(sorted(inverted_word_index.items()))\n",
        "\n",
        "print(f\"First 100 elements of the inverted_word_index: {list(inverted_word_index.items())[:100]}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 elements of the word_index: [('\\x08\\x08\\x08\\x08a', 68893), ('\\x10own', 70879), (\"'\", 755), (\"''\", 17448), (\"''a\", 57351), (\"''after\", 64015), (\"''bad\", 72030), (\"''cannibal\", 58448), (\"''clients''\", 80824), (\"''dark''\", 66872), (\"''empire\", 52876), (\"''family\", 85943), (\"''gaslight''\", 58747), (\"''heart''\", 75354), (\"''high\", 66908), (\"''holy\", 83565), (\"''human''\", 61364), (\"''humans''\", 88090), (\"''i'm\", 61351), (\"''if\", 62727), (\"''inuyasha''\", 61770), (\"''little''\", 58164), (\"''maison\", 85895), (\"''ned''\", 62202), (\"''negative''\", 76149), (\"''nice\", 57008), (\"''oh\", 45418), (\"''on\", 66910), (\"''oversexed''\", 75509), (\"''peeping\", 81867), (\"''professionals''\", 76109), (\"''ranma\", 44903), (\"''raptors''\", 70722), (\"''return\", 55138), (\"''saint\", 51394), (\"''scarface''\", 51257), (\"''sea\", 49404), (\"''talent\", 84926), (\"''terrorists''\", 81627), (\"''the\", 20197), (\"''their\", 84051), (\"''this\", 66430), (\"''thunderball\", 87604), (\"''troubled''\", 79833), (\"''unpleasant\", 66417), (\"''villain\", 71277), (\"''voyeur''\", 70370), (\"''we're\", 66244), (\"''while''\", 65559), (\"''your\", 65411), (\"''zero\", 47109), (\"'0\", 79347), (\"'00s\", 30582), (\"'01\", 24377), (\"'02\", 32858), (\"'03\", 32859), (\"'04\", 32856), (\"'05\", 38134), (\"'06\", 32857), (\"'07\", 34094), (\"'08\", 47119), (\"'1\", 79346), (\"'1'\", 39270), (\"'10\", 79074), (\"'10'\", 24957), (\"'12\", 24746), (\"'1408'\", 69312), (\"'1902'\", 65699), (\"'1909\", 60730), (\"'1940'\", 75118), (\"'2'\", 35883), (\"'2001\", 69564), (\"'20s\", 60473), (\"'24\", 87592), (\"'24'\", 40710), (\"'28\", 35882), (\"'30\", 65650), (\"'30's\", 18972), (\"'30s\", 12641), (\"'34\", 32136), (\"'38\", 65652), (\"'39\", 65651), (\"'3rd\", 73766), (\"'4\", 49196), (\"'4'\", 44164), (\"'40\", 82137), (\"'40's\", 18786), (\"'40s\", 14047), (\"'41\", 62912), (\"'42\", 28530), (\"'42nd\", 58169), (\"'43\", 44167), (\"'44\", 36549), (\"'45\", 62913), (\"'46\", 44168), (\"'47\", 62914), (\"'48\", 44169), (\"'4th\", 53118), (\"'5'\", 70028), (\"'50's\", 20059)]\n",
            "First 100 elements of the inverted_word_index: [(1, '[START]'), (2, '[OOV]'), (4, '[START]'), (5, '[OOV]'), (6, 'a'), (7, 'of'), (8, 'to'), (9, 'is'), (10, 'br'), (11, 'in'), (12, 'it'), (13, 'i'), (14, 'this'), (15, 'that'), (16, 'was'), (17, 'as'), (18, 'for'), (19, 'with'), (20, 'movie'), (21, 'but'), (22, 'film'), (23, 'on'), (24, 'not'), (25, 'you'), (26, 'are'), (27, 'his'), (28, 'have'), (29, 'he'), (30, 'be'), (31, 'one'), (32, 'all'), (33, 'at'), (34, 'by'), (35, 'an'), (36, 'they'), (37, 'who'), (38, 'so'), (39, 'from'), (40, 'like'), (41, 'her'), (42, 'or'), (43, 'just'), (44, 'about'), (45, \"it's\"), (46, 'out'), (47, 'has'), (48, 'if'), (49, 'some'), (50, 'there'), (51, 'what'), (52, 'good'), (53, 'more'), (54, 'when'), (55, 'very'), (56, 'up'), (57, 'no'), (58, 'time'), (59, 'she'), (60, 'even'), (61, 'my'), (62, 'would'), (63, 'which'), (64, 'only'), (65, 'story'), (66, 'really'), (67, 'see'), (68, 'their'), (69, 'had'), (70, 'can'), (71, 'were'), (72, 'me'), (73, 'well'), (74, 'than'), (75, 'we'), (76, 'much'), (77, 'been'), (78, 'bad'), (79, 'get'), (80, 'will'), (81, 'do'), (82, 'also'), (83, 'into'), (84, 'people'), (85, 'other'), (86, 'first'), (87, 'great'), (88, 'because'), (89, 'how'), (90, 'him'), (91, 'most'), (92, \"don't\"), (93, 'made'), (94, 'its'), (95, 'then'), (96, 'way'), (97, 'make'), (98, 'them'), (99, 'too'), (100, 'could'), (101, 'any')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9axnbnwR6q6W"
      },
      "source": [
        "# we cannot create a dataset :( this is because sequences are different length\n",
        "# but tensors have to be \"rectangular\"\n",
        "# train_data = tf.data.Dataset.from_tensor_slices(train_sequences, train_labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2lt9mE-9XO7"
      },
      "source": [
        "# solution is padding all sequences to the maximum length.\n",
        "# first find the maximum length\n",
        "seq_lengths = [len(sequence) for sequence in train_seqs]\n",
        "max_seq_length = max(seq_lengths)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "677ZXcRu9nUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "857eda5c-cb1d-43cf-a099-91071228d241"
      },
      "source": [
        "# overview over sequence lengths in the data\n",
        "# could also look at mean, median, standard deviation...\n",
        "plt.hist(seq_lengths, bins=1000)\n",
        "\n",
        "sl_np_array = np.array(seq_lengths)\n",
        "min_seq_length = np.min(sl_np_array)\n",
        "median_seq_length = np.median(sl_np_array)\n",
        "average_seq_length = np.average(sl_np_array)\n",
        "\n",
        "print(f\"Minimum text length is: {min_seq_length}\")\n",
        "print(f\"Maximum text length is: {max_seq_length}\")\n",
        "print(f\"Median text length is: {median_seq_length}\")\n",
        "print(f\"Average text length is: {average_seq_length}\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum text length is: 11\n",
            "Maximum text length is: 2494\n",
            "Median text length is: 178.0\n",
            "Average text length is: 238.71364\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYr10G5M9rWX"
      },
      "source": [
        "# luckily there is a convenient function for padding\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sqs, maxlen=max_len, padding='pre')"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXEICggj-OL-"
      },
      "source": [
        "# now we can create a dataset!\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPTPy5Ff-Q_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47367817-af48-436e-e434-cd6eb70912ec"
      },
      "source": [
        "# all sequences are... very long\n",
        "print(train_sequences_padded.shape)\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 2494)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug0OSIGjf6ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756ae69d-ad1a-411d-a1ee-8a2475035824"
      },
      "source": [
        "# it would be better to do something like this\n",
        "# all sequences above maxlen will be truncated to that length\n",
        "# note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=200)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
        "\n",
        "print(train_sequences_padded.shape)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZW7YdDv_fRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c544141-4d2a-475c-f11f-784a33632611"
      },
      "source": [
        "# for fun, you can look at the word-index mappings.\n",
        "# in this case, the mapping was done according to word frequency.\n",
        "# you can pass reverse=True to sorted() to look at the least common words.\n",
        "print(\"The most frequent 100 word are: \")\n",
        "print(sorted(inverted_word_index.items())[2:102])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent 100 word are: \n",
            "[(4, 'the'), (5, 'and'), (6, 'a'), (7, 'of'), (8, 'to'), (9, 'is'), (10, 'br'), (11, 'in'), (12, 'it'), (13, 'i'), (14, 'this'), (15, 'that'), (16, 'was'), (17, 'as'), (18, 'for'), (19, 'with'), (20, 'movie'), (21, 'but'), (22, 'film'), (23, 'on'), (24, 'not'), (25, 'you'), (26, 'are'), (27, 'his'), (28, 'have'), (29, 'he'), (30, 'be'), (31, 'one'), (32, 'all'), (33, 'at'), (34, 'by'), (35, 'an'), (36, 'they'), (37, 'who'), (38, 'so'), (39, 'from'), (40, 'like'), (41, 'her'), (42, 'or'), (43, 'just'), (44, 'about'), (45, \"it's\"), (46, 'out'), (47, 'has'), (48, 'if'), (49, 'some'), (50, 'there'), (51, 'what'), (52, 'good'), (53, 'more'), (54, 'when'), (55, 'very'), (56, 'up'), (57, 'no'), (58, 'time'), (59, 'she'), (60, 'even'), (61, 'my'), (62, 'would'), (63, 'which'), (64, 'only'), (65, 'story'), (66, 'really'), (67, 'see'), (68, 'their'), (69, 'had'), (70, 'can'), (71, 'were'), (72, 'me'), (73, 'well'), (74, 'than'), (75, 'we'), (76, 'much'), (77, 'been'), (78, 'bad'), (79, 'get'), (80, 'will'), (81, 'do'), (82, 'also'), (83, 'into'), (84, 'people'), (85, 'other'), (86, 'first'), (87, 'great'), (88, 'because'), (89, 'how'), (90, 'him'), (91, 'most'), (92, \"don't\"), (93, 'made'), (94, 'its'), (95, 'then'), (96, 'way'), (97, 'make'), (98, 'them'), (99, 'too'), (100, 'could'), (101, 'any'), (102, 'movies'), (103, 'after')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a RNN Model for IMDB Movie Review"
      ],
      "metadata": {
        "id": "rT8kelR-z0S7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "ZgvjA0Jx-cMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversions"
      ],
      "metadata": {
        "id": "F1DpKRUaBU2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indx_to_word(i: int) -> str:\n",
        "  return inverted_word_index.get(i, \"UNKNOWN\")\n",
        "\n",
        "def word_to_indx(w: str) -> int:\n",
        "  return word_index[w]+3\n",
        "\n",
        "def seq_to_words(seq: list[str]) -> str:\n",
        "  \"\"\"\n",
        "  Converts a sequence (a list of indices, decoding words) to decoded string (a list of strings the indices decode)\n",
        "  \"\"\"\n",
        "  return [indx_to_word(indx) for indx in seq]\n",
        "\n",
        "def seq_to_text(seq: list[str]) -> str:\n",
        "  \"\"\"\n",
        "  Converts a sequence (a list of indices, decoding words) to decoded human readable text (one large string)\n",
        "  \"\"\"\n",
        "  return \" \".join(seq_to_words(seq))"
      ],
      "metadata": {
        "id": "f8c5C5u9BWU5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seq_to_text(train_seqs[0]))\n",
        "print(seq_to_words(train_seqs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkOX0FNmJQg0",
        "outputId": "aa365227-b916-484e-9cca-779fd988495e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START] this film was just brilliant casting location scenery story direction everyone's really suited [START] part they played [OOV] you could just imagine being there robert [OOV] is an amazing actor [OOV] now [START] same being director [OOV] father came from [START] same scottish island as myself so i loved [START] fact there was a real connection with this film [START] witty remarks throughout [START] film were great it was just brilliant so much that i bought [START] film as soon as it was released for [OOV] [OOV] would recommend it to everyone to watch [OOV] [START] fly [OOV] was amazing really cried at [START] end it was so sad [OOV] you know what they say if you cry at a film it must have been good [OOV] this definitely was also [OOV] to [START] two little [OOV] that played [START] [OOV] of norman [OOV] paul they were just brilliant children are often left out of [START] [OOV] list i think because [START] stars that play them all grown up are such a big [OOV] for [START] whole film but these children are amazing [OOV] should be [OOV] for what they have done don't you think [START] whole story was so lovely because it was true [OOV] was someone's life after all that was [OOV] with us all\n",
            "['[START]', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', '[START]', 'part', 'they', 'played', '[OOV]', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '[OOV]', 'is', 'an', 'amazing', 'actor', '[OOV]', 'now', '[START]', 'same', 'being', 'director', '[OOV]', 'father', 'came', 'from', '[START]', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', '[START]', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', '[START]', 'witty', 'remarks', 'throughout', '[START]', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', '[START]', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '[OOV]', '[OOV]', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', '[OOV]', '[START]', 'fly', '[OOV]', 'was', 'amazing', 'really', 'cried', 'at', '[START]', 'end', 'it', 'was', 'so', 'sad', '[OOV]', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', '[OOV]', 'this', 'definitely', 'was', 'also', '[OOV]', 'to', '[START]', 'two', 'little', '[OOV]', 'that', 'played', '[START]', '[OOV]', 'of', 'norman', '[OOV]', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', '[START]', '[OOV]', 'list', 'i', 'think', 'because', '[START]', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', '[OOV]', 'for', '[START]', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', '[OOV]', 'should', 'be', '[OOV]', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', '[START]', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', '[OOV]', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', '[OOV]', 'with', 'us', 'all']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjective or Adverb Check"
      ],
      "metadata": {
        "id": "ZcJUQEsPY0qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK library has a really bad performance for detecting adjectives ~ 50%\n",
        "def is_adjective_or_adverb_using_nltk(s: str) -> bool:\n",
        "  nltk_type_of_word = nltk.pos_tag([s], tagset=\"universal\")[0][1]\n",
        "  is_adjective_or_adverb = nltk_type_of_word == \"ADJ\" or nltk_type_of_word == \"ADV\"\n",
        "  #if is_adjective_or_adverb:\n",
        "  print(f\"{s} {'is an adjective or adverb' if is_adjective_or_adverb else f'is not an adjective or adverb, it is: {nltk_type_of_word}'}\")\n",
        "  return is_adjective_or_adverb\n",
        "\n",
        "# Lemminflect's performance is > 90%, way better!\n",
        "def is_adjective_or_adverb(s: str) -> bool:\n",
        "  lemmas = getAllLemmas(s)\n",
        "  #print(f\"{s}'s lemmas: {lemmas}\")\n",
        "  return \"ADJ\" in lemmas or \"ADV\" in lemmas"
      ],
      "metadata": {
        "id": "GVK1o_SFY3Fk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data"
      ],
      "metadata": {
        "id": "sfRCNzLn-d65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process a single string\n",
        "def pre_process_str(s: str) -> str:\n",
        "\n",
        "  # Remove punctuation\n",
        "  def remove_punctuation(s: str) -> str:\n",
        "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
        "    return s\n",
        "\n",
        "  return remove_punctuation(s)\n",
        "\n",
        "\n",
        "# Pre-process an entire sequence\n",
        "def pre_process_sequence(seq: list[int]) -> list[int]:\n",
        "  words = seq_to_words(seq)\n",
        "  words_processed = [w for w in words if(w != \"UNKNOWN\" and  w != \"[OOV]\" and w != \"[START]\" and is_adjective_or_adverb(w))]\n",
        "  return [word_to_indx(w) for w in words_processed]\n",
        "\n"
      ],
      "metadata": {
        "id": "pxHamMpi2DtD"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Without processing: {seq_to_text(train_seqs[0])}\")\n",
        "print(f\"After preprocessing: {seq_to_text(pre_process_sequence(train_seqs[0]))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W83Ynq0sZ3to",
        "outputId": "6929ac97-9541-4691-f864-07e402fa3891"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without processing: [START] this film was just brilliant casting location scenery story direction everyone's really suited [START] part they played [OOV] you could just imagine being there robert [OOV] is an amazing actor [OOV] now [START] same being director [OOV] father came from [START] same scottish island as myself so i loved [START] fact there was a real connection with this film [START] witty remarks throughout [START] film were great it was just brilliant so much that i bought [START] film as soon as it was released for [OOV] [OOV] would recommend it to everyone to watch [OOV] [START] fly [OOV] was amazing really cried at [START] end it was so sad [OOV] you know what they say if you cry at a film it must have been good [OOV] this definitely was also [OOV] to [START] two little [OOV] that played [START] [OOV] of norman [OOV] paul they were just brilliant children are often left out of [START] [OOV] list i think because [START] stars that play them all grown up are such a big [OOV] for [START] whole film but these children are amazing [OOV] should be [OOV] for what they have done don't you think [START] whole story was so lovely because it was true [OOV] was someone's life after all that was [OOV] with us all\n",
            "After preprocessing: just brilliant really part just being there amazing now same being same as so there real witty throughout great just brilliant so much as soon as to to amazing really so sad good definitely also to two little just brilliant often left out up big whole amazing done whole so lovely true after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4fwUhqBACri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b5606f-aa24-49e5-9228-72d2445cafb2"
      },
      "source": [
        "# here is a high-level sketch for training RNNs\n",
        "\n",
        "# training loop -- same thing as before!!\n",
        "# our data is now slightly different (each batch of sequences has a time axis, which is kinda new)\n",
        "# but all the related changes are hidden away at lower levels\n",
        "def train_loop():\n",
        "    for sequence_batch, label_batch in train_data:\n",
        "        train_step(sequence_batch, label_batch)\n",
        "\n",
        "\n",
        "# a single training step -- again, seems familiar?\n",
        "def train_step(sequences, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = rnn_loop(sequences)\n",
        "        loss = loss_fn(labels, logits)\n",
        "\n",
        "    gradient = ...\n",
        "    apply_gradients(...)\n",
        "\n",
        "\n",
        "# here's where things start to change\n",
        "# we loop over the input time axis, and at each time step compute the new\n",
        "# hidden state based on the previous one as well as the current input\n",
        "# the state computation is hidden away in the rnn_step function and could be\n",
        "# arbitrarily complex.\n",
        "# in the general RNN, an output is computed at each time step, and the whole\n",
        "# sequence is returned. but in this case, since we only have one label for the\n",
        "# entire sequence, we only use the final state to compute one output and return it.\n",
        "# before the loop, the state need to be initialized somehow.\n",
        "@tf.function\n",
        "def rnn_loop(sequences):\n",
        "    old_state = ...  # initial state\n",
        "\n",
        "    for step in tf.range(max_len):\n",
        "        x_t = sequences[:, step]\n",
        "        x_t = tf.one_hot(x_t, depth=num_words)\n",
        "        new_state = rnn_step(old_state, x_t)\n",
        "\n",
        "        old_state = new_state\n",
        "\n",
        "    o_t = output_layer(new_state)\n",
        "\n",
        "    return o_t\n",
        "\n",
        "\n",
        "# see formulas in the book ;)\n",
        "def rnn_step(state, x_t):\n",
        "    ..."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ellipsis\n",
            "<class 'ellipsis'>\n"
          ]
        }
      ]
    }
  ]
}